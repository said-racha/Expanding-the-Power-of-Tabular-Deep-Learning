{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"14QRHTTzrehdw70vl2L2y-fz6u72wGGzg","timestamp":1737383546595}],"authorship_tag":"ABX9TyPeLmRr5DMg+oWqRpuEx2ib"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## TABM"],"metadata":{"id":"32g8j0cZUseo"}},{"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","# ===== LAYERS =====\n","\n","class linear_BE(nn.Module):\n","    def __init__(self, in_features: int, out_features: int, k=32, dropout_rate=0.1, initialize_to_1=False):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.k = k\n","\n","        if initialize_to_1:  # For TabM\n","            self.R = nn.Parameter(torch.ones(k, in_features))\n","            self.S = nn.Parameter(torch.ones(k, out_features))\n","        else:\n","            # Paper generates randomly with +-1\n","            self.R = nn.Parameter(torch.zeros((k, in_features)))\n","            nn.init.uniform_(self.R, -1, 1)\n","            self.S = nn.Parameter(torch.zeros((k, out_features)))\n","            nn.init.uniform_(self.S, -1, 1)\n","\n","        self.W = nn.Parameter(torch.zeros((in_features, out_features)))\n","        nn.init.uniform_(self.W, -1, 1)\n","        self.B = nn.Parameter(torch.zeros((k, out_features)))\n","        nn.init.uniform_(self.B, -1, 1)\n","\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, X: torch.Tensor):\n","        \"\"\"\n","        Shapes:\n","\n","        X: (batch_size, k, in_features)\n","        R: (k, in_features)\n","        W: (in_features, out_features)\n","        S: (k, out_features)\n","        B: (k, out_features)\n","        output: (batch_size, k, out_features)\n","\n","        Formula:\n","        output = ( (X * R) W) * S + B\n","        \"\"\"\n","        output = X * self.R\n","\n","        output = torch.einsum(\"bki,io->bko\", output, self.W)\n","        output = output * self.S + self.B\n","        output = self.relu(output)\n","        output = self.dropout(output)\n","\n","        return output\n","\n","    def extra_repr(self):\n","        \"\"\"\n","        Adds information about the layer to its string representation (useful when printing the model)\n","        \"\"\"\n","        return f\"in_features={self.in_features}, out_features={self.out_features}\"\n","\n","\n","class MLP_layer(nn.Module):\n","    def __init__(self, in_features: int, out_features: int, dropout_rate=0.1):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","\n","        self.linear = nn.Linear(in_features, out_features)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, X: torch.Tensor):\n","        output = self.linear(X)\n","        output = self.relu(output)\n","        output = self.dropout(output)\n","\n","        return output\n","\n","    def extra_repr(self):\n","        return f\"in_features={self.in_features}, out_features={self.out_features}\"\n","\n","\n","class MLPk_layer(nn.Module):\n","    def __init__(self, in_features: int, out_features: int, k=32, dropout_rate=0.1):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","\n","        self.W = nn.Parameter(torch.zeros((k, in_features, out_features)))\n","        nn.init.uniform_(self.W, -1, 1)\n","        self.B = nn.Parameter(torch.zeros((k, out_features)))\n","        nn.init.uniform_(self.B, -1, 1)\n","\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, X: torch.Tensor):\n","        \"\"\"\n","        Shapes:\n","\n","        X: (batch_size, k, in_features)\n","        W: (k, in_features, out_features)\n","        B: (k, out_features)\n","        output: (batch_size, k, out_features)\n","\n","        Formula:\n","        output = X @ W + B\n","        \"\"\"\n","        output = torch.einsum(\"bki,kio->bko\", X, self.W)\n","        output = output + self.B\n","\n","        output = self.relu(output)\n","        output = self.dropout(output)\n","\n","        return output\n","\n","    def extra_repr(self):\n","        return f\"in_features={self.in_features}, out_features={self.out_features}\"\n","\n","\n","# ===== BACKBONES =====\n","\n","\n","class TabM_naive(nn.Module):\n","    def __init__(self, in_features: int, hidden_sizes: int, k=32, dropout_rate=0.1):\n","        super().__init__()\n","\n","        self.in_features = in_features\n","        self.hidden_sizes = hidden_sizes\n","        self.k = k\n","\n","        layer_sizes = [in_features] + hidden_sizes\n","\n","        layers = [linear_BE(layer_sizes[i], layer_sizes[i+1], k, dropout_rate) for i in range(len(layer_sizes)-1)]\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","    def forward(self, X: torch.Tensor):\n","        return self.layers(X)\n","\n","\n","class TabM_mini(nn.Module):\n","    def __init__(self, in_features: int, hidden_sizes: int, k=32, dropout_rate=0.1):\n","        super().__init__()\n","\n","        self.k = k\n","\n","        self.R = nn.Parameter(torch.randn(k, in_features))\n","\n","        layer_sizes = [in_features] + hidden_sizes\n","\n","        layers = [MLP_layer(layer_sizes[i], layer_sizes[i+1], dropout_rate) for i in range(len(layer_sizes)-1)]\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","    def forward(self, X: torch.Tensor):\n","        output = X * self.R\n","        return self.layers(output)\n","\n","\n","class TabM(nn.Module):\n","    def __init__(self, in_features: int, hidden_sizes: int, k=32, dropout_rate=0.1):\n","        super().__init__()\n","\n","        self.k = k\n","\n","        layer_sizes = [in_features] + hidden_sizes\n","\n","        layers = [linear_BE(layer_sizes[i], layer_sizes[i+1], k, dropout_rate, initialize_to_1=True) for i in range(len(layer_sizes)-1)]\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","    def forward(self, X: torch.Tensor):\n","        return self.layers(X)\n","\n","\n","class MLPk(nn.Module):\n","    def __init__(self, in_features: int, hidden_sizes: int, k=32, dropout_rate=0.1):\n","        super().__init__()\n","\n","        layer_sizes = [in_features] + hidden_sizes\n","\n","        layers = [MLPk_layer(layer_sizes[i], layer_sizes[i+1], k, dropout_rate) for i in range(len(layer_sizes)-1)]\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","    def forward(self, X: torch.Tensor):\n","        return self.layers(X)\n","\n","# ===== MODELS =====\n","\n","\n","class MLP(nn.Module):\n","    \"\"\"\n","    Simple MLP model\n","    \"\"\"\n","\n","    def __init__(self, in_features: int, hidden_sizes: int, out_features: int, dropout_rate=0.1):\n","        super().__init__()\n","\n","        self.in_features = in_features\n","        self.out_features = out_features\n","\n","        layer_sizes = [in_features] + hidden_sizes + [out_features]\n","\n","        layers = [*[MLP_layer(layer_sizes[i], layer_sizes[i+1], dropout_rate) for i in range(len(layer_sizes)-1)],\n","                  nn.Linear(layer_sizes[-1], out_features)\n","                  ]\n","\n","        self.layers = nn.Sequential(*layers)\n","\n","    def forward(self, X: torch.Tensor):\n","        return self.layers(X)\n","\n","\n","class EnsembleModel(nn.Module):\n","    \"\"\"\n","    Global ensemble model that :\n","    - takes batched input (batch, in_features)\n","    - clones it k times (batch, k, in_features)\n","    - passes it through a backbone (which model you want e.g TabM, MLPk, etc.) (batch, k, hidden_sizes[-1])\n","    - passes the output through k prediction heads, mean over heads (batch, out_features)\n","    \"\"\"\n","\n","    def __init__(self, backbone: nn.Module, in_features: int, hidden_sizes: int, out_features: int, k=32, dropout_rate=0.1, mean_over_heads=True):\n","        super().__init__()\n","\n","        self.backbone = backbone(in_features, hidden_sizes, k, dropout_rate)\n","        self.in_features = in_features\n","        self.k = k\n","\n","        self.mean_over_heads = mean_over_heads\n","\n","        self.pred_heads = nn.ModuleList([nn.Linear(hidden_sizes[-1], out_features) for _ in range(k)])\n","\n","    def forward(self, X: torch.Tensor):\n","        # clone X to shape (batch, k, dim)\n","        X = X.unsqueeze(1).repeat(1, self.k, 1)\n","\n","        # pass through backbone\n","        X = self.backbone(X)\n","\n","        # pass through prediction heads\n","        preds = [head(X[:, i]) for i, head in enumerate(self.pred_heads)]\n","\n","        # concatenate head predictions\n","        preds = torch.stack(preds, dim=1)\n","\n","        if self.mean_over_heads:\n","            preds = preds.mean(dim=1)\n","\n","        return preds\n","\n"],"metadata":{"id":"IX9-7fhJUvzi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prunning"],"metadata":{"id":"yCcK7kxiIca-"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class PrunableEnsembleModel(nn.Module):\n","    \"\"\"\n","    Ensemble model avec mécanisme de pruning :\n","    - Mesure la contribution individuelle de chaque sous-modèle.\n","    - Permet de supprimer progressivement les sous-modèles les moins performants.\n","    \"\"\"\n","    def __init__(self, backbone: nn.Module, in_features: int, hidden_sizes: int, out_features: int, k=32, dropout_rate=0.1):\n","        super().__init__()\n","        self.backbone = backbone(in_features, hidden_sizes, k, dropout_rate)\n","        self.in_features = in_features\n","        self.hidden_sizes = hidden_sizes\n","        self.k = k\n","        self.out_features = out_features\n","\n","        # Prédiction par sous-modèle (on ne moyenne pas contrairement à ce qu'on fait dans la classe EnsembleModel)\n","        self.pred_heads = nn.ModuleList([nn.Linear(hidden_sizes[-1], out_features) for _ in range(k)])\n","\n","    def forward(self, X: torch.Tensor):\n","        \"\"\"\n","        Forward pass :\n","        - Les prédictions de chaque sous-modèle sont concaténées.\n","        \"\"\"\n","        # Répliquer X pour chaque sous-modèle\n","        X = X.unsqueeze(1).repeat(1, self.k, 1)\n","\n","        # Passer par le backbone\n","        X = self.backbone(X)  # (batch, k, hidden_sizes[-1])\n","\n","        # Prédictions de chaque sous-modèle\n","        preds = [head(X[:, i]) for i, head in enumerate(self.pred_heads)]\n","        preds = torch.stack(preds, dim=1)  # (batch, k, out_features)\n","\n","        return preds\n","\n","    def prune(self, X: torch.Tensor, y: torch.Tensor, keep_ratio=0.5, task_type=\"binary\"):\n","      \"\"\"\n","      Pruning des sous-modèles :\n","      - Garde un ratio donné des sous-modèles avec les meilleures performances.\n","      - Suppression des sous-modèles moins performants.\n","      - S'adapte aux tâches de classification binaire, multiclass ou de régression.\n","\n","      Args:\n","          X : Entrée des données.\n","          y : Labels ou valeurs cibles.\n","          keep_ratio : Ratio de sous-modèles à conserver.\n","          task_type : Type de tâche ('binary', 'multiclass', ou 'regression').\n","      \"\"\"\n","      if not (0 < keep_ratio <= 1):\n","          raise ValueError(\"keep_ratio doit être un float entre 0 et 1.\")\n","\n","      with torch.no_grad():\n","\n","          if task_type == \"binary\":\n","              criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n","              y = y.float().view(-1, 1)\n","          elif task_type == \"multiclass\":\n","              criterion = nn.CrossEntropyLoss(reduction=\"none\")\n","          elif task_type == \"regression\":\n","              criterion = nn.MSELoss(reduction=\"none\")\n","          else:\n","              raise ValueError(\"task_type must be one of 'binary', 'multiclass', or 'regression'.\")\n","\n","          # Loss\n","          losses = []\n","          preds = self.forward(X)  # (batch, k, out_features)\n","\n","          for i in range(self.k):\n","              if task_type == \"multiclass\":\n","                  loss = criterion(preds[:, i, :], y)\n","              else:\n","                  loss = criterion(preds[:, i, :].reshape(-1, 1), y.reshape(-1, 1))\n","              losses.append(loss.mean().item())  # Moyenne de la perte pour chaque sous-modèle\n","\n","          # Trier les sous-modèles par perte (loss croissante)\n","          sorted_indices = sorted(range(self.k), key=lambda i: losses[i])\n","          keep_count = max(1, int(self.k * keep_ratio))\n","          keep_indices = sorted_indices[:keep_count]\n","\n","          # Mettre à jour les sous-modèles et leurs paramètres\n","          self.pred_heads = nn.ModuleList([self.pred_heads[i] for i in keep_indices])\n","          self.k = keep_count\n","\n","          # Pruning des paramètres du backbone\n","          for layer in self.backbone.layers:\n","              if hasattr(layer, \"R\") and hasattr(layer, \"S\") and hasattr(layer, \"B\"):\n","                  layer.R = nn.Parameter(layer.R[keep_indices])\n","                  layer.S = nn.Parameter(layer.S[keep_indices])\n","                  layer.B = nn.Parameter(layer.B[keep_indices])\n","\n","          # print(f\"Pruning effectué : {len(sorted_indices) - keep_count} sous-modèles supprimés. {self.k} restants.\")\n"],"metadata":{"id":"27Kz1Ut3vtcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_pruning(model, train_loader, keep_ratios, task_type=\"binary\"):\n","    \"\"\"\n","    Test le mécanisme de pruning.\n","    - Réduit progressivement les sous-modèles.\n","    - Mesure la précision globale après chaque étape.\n","\n","    Args:\n","        model : Modèle prunable.\n","        train_loader : DataLoader pour les données de test.\n","        keep_ratios : Liste des ratios de sous-modèles à conserver.\n","        task_type : Type de tâche ('binary', 'multiclass', ou 'regression').\n","    \"\"\"\n","\n","    for keep_ratio in keep_ratios:\n","        print(f\"\\nPruning avec keep_ratio = {keep_ratio}\")\n","\n","        # Pruning (utiliser un batch pour sélectionner les sous-modèles)\n","        for X_batch, y_batch in train_loader:\n","            model.prune(X_batch, y_batch, keep_ratio, task_type=task_type)\n","            break\n","\n","        # Évaluation des performances après pruning\n","        total_correct = 0\n","        total_samples = 0\n","        total_loss = 0.0\n","\n","        with torch.no_grad():\n","            for X_batch, y_batch in train_loader:\n","                preds = model.forward(X_batch)  # Prédictions des sous-modèles restants\n","\n","                if task_type == \"binary\":\n","                    preds = preds.mean(dim=1)  # Moyenne sur les sous-modèles\n","                    preds_binary = torch.sigmoid(preds)\n","                    preds_binary = (preds_binary > 0.5)\n","                    total_correct += (preds_binary == y_batch.reshape(-1,1)).sum().item()\n","\n","                elif task_type == \"multiclass\":\n","                    preds = preds.mean(dim=1)  # Moyenne sur les sous-modèles\n","                    preds_class = preds.argmax(dim=1)  # Trouver la classe avec la probabilité la plus élevée\n","                    total_correct += (preds_class == y_batch).sum().item()\n","                elif task_type == \"regression\":\n","                    preds = preds.mean(dim=1)  # Moyenne sur les sous-modèles\n","                    total_loss += nn.functional.mse_loss(preds, y_batch).item()\n","\n","                total_samples += y_batch.size(0)\n","\n","        # Calcul des métriques\n","        if task_type in [\"binary\", \"multiclass\"]:\n","            accuracy = total_correct / total_samples\n","            print(f\"Précision globale après pruning : {accuracy:.4f}\")\n","        elif task_type == \"regression\":\n","            avg_loss = total_loss / total_samples\n","            print(f\"Loss moyenne après pruning : {avg_loss:.4f}\")\n","\n"],"metadata":{"id":"figZxvIOvLaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy  # Import copy module for deep copying\n","\n","def train_with_pruning(\n","    model, train_loader, test_loader, optimizer, criterion,\n","    epochs=10, keep_ratios=[1.0, 0.75, 0.5, 0.25], filepath=\"best_model.pth\",\n","    dataset_name=\"\", task_type=\"binary\"\n","):\n","    \"\"\"\n","    Entraîne un modèle avec un mécanisme de pruning progressif.\n","\n","    Args:\n","        model : Modèle prunable.\n","        train_loader : DataLoader pour les données d'entraînement.\n","        test_loader : DataLoader pour les données de test.\n","        optimizer : Optimiseur.\n","        criterion : Fonction de perte.\n","        epochs : Nombre d'époques d'entraînement.\n","        keep_ratios : Liste des ratios de sous-modèles à conserver.\n","        filepath : Chemin pour sauvegarder le meilleur modèle.\n","        dataset_name : Nom du dataset utilisé.\n","        task_type : Type de tâche ('binary', 'multiclass', ou 'regression').\n","\n","    Returns:\n","        model : Le meilleur modèle entraîné.\n","        best_config : Configuration associée au meilleur modèle.\n","    \"\"\"\n","    best_loss = float('inf')\n","    best_model = None\n","    best_config = {}  # Sauvegarder la configuration du meilleur modèle\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        epoch_loss = 0.0\n","\n","        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n","            optimizer.zero_grad()\n","            outputs = model(X_batch)\n","\n","            if task_type == task_type:\n","                preds = outputs.mean(dim=1)\n","                loss = criterion(preds, y_batch.unsqueeze(1).float())\n","\n","            elif task_type == task_type:\n","\n","                preds = outputs.mean(dim=1)  # Moyenne sur les sous-modèles\n","                preds_class = preds.argmax(dim=1)  # Trouver la classe avec la probabilité la plus élevée\n","                loss = criterion(preds_class, y_batch)\n","\n","            elif task_type == task_type:\n","\n","                # loss = criterion(preds, y_batch.float())\n","\n","                preds = outputs.mean(dim=1)  # Moyenne sur les sous-modèles\n","                loss += nn.functional.mse_loss(preds, y_batch).item()\n","\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        avg_epoch_loss = epoch_loss / len(train_loader)\n","        print(f\"Epoch {epoch+1}, Loss: {avg_epoch_loss}\")\n","\n","        # Tester différents niveaux de pruning\n","        for keep_ratio in keep_ratios:\n","            model_copy = copy.deepcopy(model)\n","            model_copy.prune(X_batch, y_batch, keep_ratio=keep_ratio, task_type=task_type)\n","\n","            model_copy.eval()\n","            total_loss = 0.0\n","            total_correct = 0\n","            total_samples = 0\n","\n","            with torch.no_grad():\n","                for X_batch, y_batch in test_loader:\n","                    outputs = model_copy(X_batch)\n","\n","                    if task_type == \"binary\":\n","                        preds = outputs.mean(dim=1)\n","                        loss = criterion(preds, y_batch.unsqueeze(1).float())\n","                        # Calculer l'accuracy\n","                        probs = torch.sigmoid(preds)\n","                        predictions = (probs >= 0.5).float()\n","                        total_correct +=  (predictions == y_batch.unsqueeze(1)).float().sum()\n","\n","\n","                    elif task_type == \"multiclass\":\n","                        preds = outputs.mean(dim=1)  # Moyenne sur les sous-modèles\n","                        preds_class = preds.argmax(dim=1)  # Trouver la classe avec la probabilité la plus élevée\n","                        loss = criterion(preds_class, y_batch)\n","\n","                        total_correct += (preds_class == y_batch).sum().item()\n","\n","                    elif task_type == \"regression\":\n","                        preds = outputs.mean(dim=1)\n","                        preds = outputs.mean(dim=1)  # Moyenne sur les sous-modèles\n","                        loss += nn.functional.mse_loss(preds, y_batch).item()\n","\n","\n","                    total_loss += loss.item()\n","                    total_samples += y_batch.size(0)\n","\n","            avg_loss = total_loss / len(test_loader)\n","\n","            if task_type in [\"binary\", \"multiclass\"]:\n","                accuracy = total_correct / total_samples\n","                print(f\"Pruning with keep_ratio={keep_ratio}, Accuracy: {accuracy:.4f}\")\n","            elif task_type == \"regression\":\n","                print(f\"Pruning with keep_ratio={keep_ratio}, Test Loss: {avg_loss:.4f}\")\n","\n","            if avg_loss < best_loss:\n","                best_loss = avg_loss\n","                best_model = model_copy\n","                best_config = {\"k\": model_copy.k}\n","\n","        # Charger le meilleur modèle pour la prochaine epoch\n","        model = best_model\n","\n","    return model, best_config\n"],"metadata":{"id":"NrcbiTPRf2ac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_model(model, filepath, config):\n","    \"\"\"\n","    Sauvegarde un modèle PyTorch et sa configuration.\n","\n","    Args:\n","        model (nn.Module): Le modèle à sauvegarder.\n","        filepath (str): Chemin vers le fichier de sauvegarde.\n","        config (dict): Configuration du modèle (par exemple, k après pruning).\n","    \"\"\"\n","    # Include all necessary arguments in the config dictionary\n","    config.update({\n","        \"backbone\": model.backbone.__class__,  # Get the class of the backbone\n","        \"in_features\": model.in_features,\n","        \"hidden_sizes\": model.hidden_sizes,\n","        \"out_features\": model.out_features,\n","        \"k\": model.k  # Make sure k is also included\n","    })\n","\n","    torch.save({\n","        \"state_dict\": model.state_dict(),\n","        \"config\": config\n","    }, filepath)\n","    print(f\"Modèle et configuration sauvegardés dans {filepath}\")\n","\n","def load_model(filepath, model_class, default_config):\n","    \"\"\"\n","    Charge un modèle PyTorch à partir d'un fichier.\n","\n","    Args:\n","        filepath (str): Chemin vers le fichier du modèle sauvegardé.\n","        model_class (nn.Module): La classe du modèle à charger.\n","        default_config (dict): Configuration par défaut du modèle.\n","\n","    Returns:\n","        nn.Module: Le modèle chargé et prêt à être utilisé.\n","    \"\"\"\n","    # Charger le fichier\n","    checkpoint = torch.load(filepath)  # Remove weights_only=True\n","    state_dict = checkpoint[\"state_dict\"]\n","    config = checkpoint.get(\"config\", default_config)  # Utiliser la configuration sauvegardée ou par défaut\n","\n","    # Réinitialiser le modèle avec la bonne configuration\n","    model = model_class(**config)\n","\n","    # Charger les paramètres du modèle\n","    model.load_state_dict(state_dict)\n","\n","    # Passer le modèle en mode évaluation\n","    model.eval()\n","\n","    print(f\"Modèle chargé depuis {filepath}\")\n","    return model"],"metadata":{"id":"1x1DVaZxnRNi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"QxuZ2VB5Uay5"}},{"cell_type":"markdown","source":["### Dataset \"Breast cancer\""],"metadata":{"id":"tBgI3oIvUzEs"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Charger les données\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","# Diviser les données en ensembles d'entraînement et de test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Normaliser les données\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Convertir en tenseurs PyTorch\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_test = torch.tensor(y_test, dtype=torch.float32)\n","\n","# Créer des DataLoader\n","train_dataset = TensorDataset(X_train, y_train)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","test_dataset = TensorDataset(X_test, y_test)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"],"metadata":{"id":"6hHU4KGJfQSX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialisation du modèle\n","layers = [64, 32, 16]\n","prunable_model = PrunableEnsembleModel(TabM, in_features=X_train.shape[1], hidden_sizes=layers, out_features=1, k=32)\n","\n","# Test du pruning\n","keep_ratios = [1, 0.75, 0.5, 0.25, 0.1]  # Réduction progressive\n","test_pruning(prunable_model, train_loader, keep_ratios)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pojSdaO5vR1N","executionInfo":{"status":"ok","timestamp":1738023980409,"user_tz":-60,"elapsed":8,"user":{"displayName":"racha said","userId":"14947389989367359459"}},"outputId":"2e6d78ed-7f8b-4ff7-86c2-d2211c754685"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Pruning avec keep_ratio = 1\n","Précision globale après pruning : 0.4066\n","\n","Pruning avec keep_ratio = 0.75\n","Précision globale après pruning : 0.5934\n","\n","Pruning avec keep_ratio = 0.5\n","Précision globale après pruning : 0.5956\n","\n","Pruning avec keep_ratio = 0.25\n","Précision globale après pruning : 0.4989\n","\n","Pruning avec keep_ratio = 0.1\n","Précision globale après pruning : 0.5934\n"]}]},{"cell_type":"markdown","source":["#### Entrainement sur plusieurs epochs"],"metadata":{"id":"hB1hdS5LkEra"}},{"cell_type":"code","source":["\n","# Initialiser le modèle\n","model = PrunableEnsembleModel(TabM, in_features=X_train.shape[1], hidden_sizes=[64, 32], out_features=1, k=32)\n","\n","# Définir l'optimiseur et la fonction de perte\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.BCEWithLogitsLoss()\n","\n","filepath = \"best_model.pth\"\n","dataset_name = \"cancer\"\n","\n","# Entraîner le modèle avec pruning\n","trained_model, best_config = train_with_pruning(model, train_loader, test_loader, optimizer, criterion, epochs=10)\n","\n","# Sauvegarder le modèle et sa configuration\n","model_name = dataset_name+\"_best_model.pth\"\n","save_model(trained_model, model_name, best_config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMXwJZh1ipoB","executionInfo":{"status":"ok","timestamp":1738023981572,"user_tz":-60,"elapsed":1169,"user":{"displayName":"racha said","userId":"14947389989367359459"}},"outputId":"718ec1fd-504e-402f-c46e-199c7326c7f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10: 100%|██████████| 15/15 [00:00<00:00, 71.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.556146643559138\n","Pruning with keep_ratio=1.0, Accuracy: 0.9737\n","Pruning with keep_ratio=0.75, Accuracy: 0.9649\n","Pruning with keep_ratio=0.5, Accuracy: 0.9561\n","Pruning with keep_ratio=0.25, Accuracy: 0.8947\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10: 100%|██████████| 15/15 [00:00<00:00, 215.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Loss: 0.35309741894404095\n","Pruning with keep_ratio=1.0, Accuracy: 0.8947\n","Pruning with keep_ratio=0.75, Accuracy: 0.9035\n","Pruning with keep_ratio=0.5, Accuracy: 0.8596\n","Pruning with keep_ratio=0.25, Accuracy: 0.8509\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10: 100%|██████████| 15/15 [00:00<00:00, 341.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Loss: 0.32457735439141594\n","Pruning with keep_ratio=1.0, Accuracy: 0.9035\n","Pruning with keep_ratio=0.75, Accuracy: 0.9298\n","Pruning with keep_ratio=0.5, Accuracy: 0.8684\n","Pruning with keep_ratio=0.25, Accuracy: 0.7368\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10: 100%|██████████| 15/15 [00:00<00:00, 375.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Loss: 0.2928622752428055\n","Pruning with keep_ratio=1.0, Accuracy: 0.9298\n","Pruning with keep_ratio=0.75, Accuracy: 0.9123\n","Pruning with keep_ratio=0.5, Accuracy: 0.9211\n","Pruning with keep_ratio=0.25, Accuracy: 0.8246\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10: 100%|██████████| 15/15 [00:00<00:00, 369.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Loss: 0.29867383738358816\n","Pruning with keep_ratio=1.0, Accuracy: 0.9298\n","Pruning with keep_ratio=0.75, Accuracy: 0.9035\n","Pruning with keep_ratio=0.5, Accuracy: 0.8772\n","Pruning with keep_ratio=0.25, Accuracy: 0.8246\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10: 100%|██████████| 15/15 [00:00<00:00, 359.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, Loss: 0.2947486917177836\n","Pruning with keep_ratio=1.0, Accuracy: 0.9298\n","Pruning with keep_ratio=0.75, Accuracy: 0.8860\n","Pruning with keep_ratio=0.5, Accuracy: 0.8772\n","Pruning with keep_ratio=0.25, Accuracy: 0.8596\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10: 100%|██████████| 15/15 [00:00<00:00, 387.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, Loss: 0.2886879285176595\n","Pruning with keep_ratio=1.0, Accuracy: 0.9298\n","Pruning with keep_ratio=0.75, Accuracy: 0.8860\n","Pruning with keep_ratio=0.5, Accuracy: 0.9211\n","Pruning with keep_ratio=0.25, Accuracy: 0.8596\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10: 100%|██████████| 15/15 [00:00<00:00, 370.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, Loss: 0.3113703628381093\n","Pruning with keep_ratio=1.0, Accuracy: 0.9298\n","Pruning with keep_ratio=0.75, Accuracy: 0.9123\n","Pruning with keep_ratio=0.5, Accuracy: 0.9211\n","Pruning with keep_ratio=0.25, Accuracy: 0.8596\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10: 100%|██████████| 15/15 [00:00<00:00, 383.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, Loss: 0.2977165271838506\n","Pruning with keep_ratio=1.0, Accuracy: 0.9298\n","Pruning with keep_ratio=0.75, Accuracy: 0.9123\n","Pruning with keep_ratio=0.5, Accuracy: 0.9211\n","Pruning with keep_ratio=0.25, Accuracy: 0.8596\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10: 100%|██████████| 15/15 [00:00<00:00, 359.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, Loss: 0.3089119285345078\n","Pruning with keep_ratio=1.0, Accuracy: 0.9298\n","Pruning with keep_ratio=0.75, Accuracy: 0.9123\n","Pruning with keep_ratio=0.5, Accuracy: 0.9211\n","Pruning with keep_ratio=0.25, Accuracy: 0.8596\n","Modèle et configuration sauvegardés dans cancer_best_model.pth\n"]}]},{"cell_type":"code","source":["\n","# Charger le modèle\n","default_config = {\n","    \"backbone\": TabM,\n","    \"in_features\": X_train.shape[1],\n","    \"hidden_sizes\": [64, 32, 16],\n","    \"out_features\": 1,\n","    \"dropout_rate\": 0.1\n","}\n","\n","default_config.update(best_config)\n","\n","loaded_model = load_model(model_name, PrunableEnsembleModel, default_config)\n","\n","# Tester le modèle chargé\n","loaded_model.eval()\n","total_correct = 0\n","total_samples = 0\n","with torch.no_grad():\n","    for X_batch, y_batch in test_loader:\n","        outputs = loaded_model(X_batch)\n","        preds = torch.sigmoid(outputs.mean(dim=1)) > 0.5\n","        total_correct += (preds == y_batch.unsqueeze(1)).sum().item()\n","        total_samples += y_batch.size(0)\n","\n","accuracy = total_correct / total_samples\n","print(f\"Test Accuracy du modèle chargé: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nEjH-YivlgXz","executionInfo":{"status":"ok","timestamp":1738023981572,"user_tz":-60,"elapsed":17,"user":{"displayName":"racha said","userId":"14947389989367359459"}},"outputId":"7f727566-5621-4437-a1de-489c6db5cffb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Modèle chargé depuis cancer_best_model.pth\n","Test Accuracy du modèle chargé: 0.9298\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-57-10735babbc9f>:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(filepath)  # Remove weights_only=True\n"]}]},{"cell_type":"markdown","source":["### Dataset \"Adult income\""],"metadata":{"id":"yQl7yyrffvQx"}},{"cell_type":"code","source":["import pandas as pd\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import fetch_openml\n","from sklearn.preprocessing import LabelEncoder\n","\n","import torch.nn as nn\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","\n","def get_adult_income_data(split=0.2, batch_size=32, seed=42):\n","    \"\"\"\n","    Chargement et préparation des données pour la classification `adult income`\n","    \"\"\"\n","    data = pd.read_csv(\"adult.csv\")\n","    X = data.drop(columns='income')\n","    y = data['income']  # target\n","\n","    X = pd.get_dummies(X)\n","\n","    le = LabelEncoder()\n","    y = le.fit_transform(y)\n","\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(X)\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=seed)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        torch.utils.data.TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).long()),\n","        batch_size=batch_size, shuffle=True\n","    )\n","    test_loader = torch.utils.data.DataLoader(\n","        torch.utils.data.TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).long()),\n","        batch_size=batch_size, shuffle=False\n","    )\n","    shape_x = X_train.shape[1]\n","    shape_y = y_train.reshape(-1,1).shape[1]\n","    return train_loader, test_loader, shape_x, shape_y\n"],"metadata":{"id":"g771-Wsc0120"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Données\n","BATCH_SIZE = 32\n","train_loader, test_loader, shape_x, shape_y = get_adult_income_data(split=.2, batch_size=BATCH_SIZE, seed=42)\n","\n","# Modèle\n","layers = [64, 64]\n","prunable_model = PrunableEnsembleModel(TabM, in_features=shape_x, hidden_sizes=layers, out_features=1, k=32)\n","\n","# Test du pruning\n","keep_ratios = [1, 0.75, 0.5, 0.25, 0.1]  # Réduction progressive du nombre de sous modèles\n","test_pruning(prunable_model, train_loader, keep_ratios, task_type=\"binary\")\n"],"metadata":{"id":"OcI2A-HwxELR","executionInfo":{"status":"ok","timestamp":1738023994765,"user_tz":-60,"elapsed":13200,"user":{"displayName":"racha said","userId":"14947389989367359459"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f531167-3269-4f80-98aa-0589da71fe81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Pruning avec keep_ratio = 1\n","Précision globale après pruning : 0.3225\n","\n","Pruning avec keep_ratio = 0.75\n","Précision globale après pruning : 0.6920\n","\n","Pruning avec keep_ratio = 0.5\n","Précision globale après pruning : 0.7576\n","\n","Pruning avec keep_ratio = 0.25\n","Précision globale après pruning : 0.7382\n","\n","Pruning avec keep_ratio = 0.1\n","Précision globale après pruning : 0.6939\n"]}]},{"cell_type":"markdown","source":["#### Entrainement sur plusieurs epochs"],"metadata":{"id":"yrzMdNhX1bse"}},{"cell_type":"code","source":["# Charger et prétraiter les données\n","train_loader, test_loader, shape_x, shape_y = get_adult_income_data(split=0.2, batch_size=32, seed=42)\n","\n","# Afficher les dimensions des données\n","print(f\"Nombre de caractéristiques (shape_x) : {shape_x}\")\n","print(f\"Nombre de classes (shape_y) : {shape_y}\")\n","\n","model = PrunableEnsembleModel(TabM, in_features=shape_x, hidden_sizes=[64, 32], out_features=shape_y, k=32)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1w7-7sAGw7rK","executionInfo":{"status":"ok","timestamp":1738023995095,"user_tz":-60,"elapsed":334,"user":{"displayName":"racha said","userId":"14947389989367359459"}},"outputId":"f0b7d019-09b2-42b8-8778-03a715ead96f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Nombre de caractéristiques (shape_x) : 108\n","Nombre de classes (shape_y) : 1\n"]}]},{"cell_type":"code","source":["# Définir l'optimiseur et la fonction de perte\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","criterion = torch.nn.BCEWithLogitsLoss()  # Pour la classification binaire\n","\n","# Entraîner le modèle avec pruning\n","trained_model, best_config = train_with_pruning(model, train_loader, test_loader, optimizer, criterion, epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUG6kgwdxybv","executionInfo":{"status":"ok","timestamp":1738024154994,"user_tz":-60,"elapsed":159904,"user":{"displayName":"racha said","userId":"14947389989367359459"}},"outputId":"8219e180-fb5e-4b6c-d468-203f8c4ea1f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10: 100%|██████████| 1222/1222 [00:19<00:00, 63.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.49272613635868956\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.7527\n","Pruning with keep_ratio=0.5, Accuracy: 0.7574\n","Pruning with keep_ratio=0.25, Accuracy: 0.7065\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10: 100%|██████████| 1222/1222 [00:14<00:00, 86.66it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Loss: 0.4192813226248941\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.7532\n","Pruning with keep_ratio=0.5, Accuracy: 0.7025\n","Pruning with keep_ratio=0.25, Accuracy: 0.7311\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10: 100%|██████████| 1222/1222 [00:12<00:00, 97.05it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Loss: 0.419344790401806\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.6934\n","Pruning with keep_ratio=0.5, Accuracy: 0.7169\n","Pruning with keep_ratio=0.25, Accuracy: 0.7488\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10: 100%|██████████| 1222/1222 [00:12<00:00, 98.34it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Loss: 0.41885892923695756\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.7812\n","Pruning with keep_ratio=0.5, Accuracy: 0.7609\n","Pruning with keep_ratio=0.25, Accuracy: 0.7054\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10: 100%|██████████| 1222/1222 [00:12<00:00, 99.13it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Loss: 0.41880364399007025\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.6944\n","Pruning with keep_ratio=0.5, Accuracy: 0.7608\n","Pruning with keep_ratio=0.25, Accuracy: 0.7535\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10: 100%|██████████| 1222/1222 [00:12<00:00, 98.00it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6, Loss: 0.41804119725079075\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.7071\n","Pruning with keep_ratio=0.5, Accuracy: 0.7299\n","Pruning with keep_ratio=0.25, Accuracy: 0.7557\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10: 100%|██████████| 1222/1222 [00:12<00:00, 97.25it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7, Loss: 0.4189207910052493\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.6540\n","Pruning with keep_ratio=0.5, Accuracy: 0.7172\n","Pruning with keep_ratio=0.25, Accuracy: 0.7710\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10: 100%|██████████| 1222/1222 [00:18<00:00, 66.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8, Loss: 0.421607683338042\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.7494\n","Pruning with keep_ratio=0.5, Accuracy: 0.7236\n","Pruning with keep_ratio=0.25, Accuracy: 0.7688\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10: 100%|██████████| 1222/1222 [00:12<00:00, 98.70it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9, Loss: 0.4175804953674638\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.7534\n","Pruning with keep_ratio=0.5, Accuracy: 0.7235\n","Pruning with keep_ratio=0.25, Accuracy: 0.7327\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10: 100%|██████████| 1222/1222 [00:12<00:00, 99.70it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10, Loss: 0.42010032670929076\n","Pruning with keep_ratio=1.0, Accuracy: 0.8249\n","Pruning with keep_ratio=0.75, Accuracy: 0.6887\n","Pruning with keep_ratio=0.5, Accuracy: 0.7696\n","Pruning with keep_ratio=0.25, Accuracy: 0.7244\n"]}]},{"cell_type":"code","source":["dataset_name = \"adult\"\n","# Sauvegarder le modèle et sa configuration\n","model_name = dataset_name+\"_best_model.pth\"\n","save_model(trained_model, model_name, best_config)\n"],"metadata":{"id":"J2DQhkXgSYpQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738024154994,"user_tz":-60,"elapsed":10,"user":{"displayName":"racha said","userId":"14947389989367359459"}},"outputId":"07cc9f26-84c9-4c6d-9589-f7b1415a2a60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Modèle et configuration sauvegardés dans adult_best_model.pth\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GlvMqLyQ17Rq"},"execution_count":null,"outputs":[]}]}